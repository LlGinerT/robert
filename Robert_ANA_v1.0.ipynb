{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import shap\n",
    "from Robert_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some information about file names for the databases and random seeds used for the RF model\n",
    "\n",
    "w_dir = os.getcwd()\n",
    "\n",
    "# name of the csv containing the database generated previously by Robert, without the CSV extension. \n",
    "# For example: csv_name = 'Phenolic_data_final_dataset' \n",
    "# csv_training = 'CSV_NAME_final_dataset'\n",
    "csv_training = 'Robert_example_final_dataset'\n",
    "\n",
    "# if there is an external set available, specify\n",
    "# csv_test = 'CSV_TEST_NAME'\n",
    "csv_test = 'Robert_example_test'\n",
    "\n",
    "# csv file containing the optimal parameters for the ML model generated previously by Robert\n",
    "csv_pred_params = 'Predictor_parameters'\n",
    "\n",
    "# specify the response value (y), for example: response_value = 'activation_barrier_kcal/mol'\n",
    "# response_value = 'YOUR_Y_VALUE'\n",
    "response_value = 'Target_values'\n",
    "\n",
    "# specify columns of the csv to drop from the descriptors but to keep in the final database\n",
    "# (i.e. reaction names). For example: fixed_descriptors = ['Name','SMILES','YSI/MW','YSI','CN','MW','weakest_bondtype']\n",
    "# fixed_descriptors = ['DESC1','DESC2','ETC']\n",
    "fixed_descriptors = ['Name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the RF parameters and dataset\n",
    "\n",
    "# load the RF parameters\n",
    "\n",
    "# name of the file containing the parameters\n",
    "parameters_df = pd.read_csv(csv_pred_params+'.csv')\n",
    "    \n",
    "# Retrieve the optimal number for the different parameters\n",
    "n_estimators = int(parameters_df['n_estimators'][0])\n",
    "max_depth = int(parameters_df['max_depth'][0])\n",
    "max_features = int(parameters_df['max_features'][0])\n",
    "train_proportion = float(parameters_df['train_proportion'][0].split('%')[0])\n",
    "model_type = parameters_df['model_type'][0]\n",
    "prediction_type = parameters_df['prediction_type'][0]\n",
    "random_init = parameters_df['random_init'][0]\n",
    "train_proportion = int(parameters_df['train_proportion'][0].split('%')[0])\n",
    "\n",
    "print('\\nThe best parameters for the random forest model are:',\n",
    "     '\\nNumber of estimators:',str(n_estimators),\n",
    "     '\\nMax. depth:',str(max_depth),\n",
    "     '\\nMax. features:',str(max_features)) \n",
    "\n",
    "# load the dataset\n",
    "\n",
    "df_model = pd.read_csv(csv_training+'.csv')\n",
    "\n",
    "training_data = df_model[df_model.Set == 'Training']\n",
    "validation_data = df_model[df_model.Set == 'Validation']\n",
    "\n",
    "X_train = training_data.drop(fixed_descriptors+[response_value]+['Set']+[f'Predicted {response_value}'], axis=1)\n",
    "X_validation = validation_data.drop(fixed_descriptors+[response_value]+['Set']+[f'Predicted {response_value}'], axis=1)\n",
    "\n",
    "y_train = training_data[response_value]\n",
    "y_validation = validation_data[response_value]\n",
    "\n",
    "print('\\nAmount of loaded datapoints for the model:',\n",
    "     '\\nTraining set:',len(training_data),'points (', round((len(training_data)/len(df_model))*100,2),'%)',\n",
    "     '\\nValidation set:',len(validation_data),'points (', round((len(validation_data)/len(df_model))*100,2),'%)') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis of the most important features and the impact they have on the predicted values\n",
    "\n",
    "# standardizes the data sets using the mean and standard dev from the train set\n",
    "Xmean = X_train.mean(axis=0)\n",
    "Xstd = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - Xmean) / Xstd\n",
    "\n",
    "if max_features <= len(X_train.columns):\n",
    "    RF = RandomForestRegressor(random_state=random_init,\n",
    "        n_estimators=n_estimators, max_features=max_features,\n",
    "        max_depth=max_depth)\n",
    "else:   \n",
    "    RF = RandomForestRegressor(random_state=random_init,\n",
    "        n_estimators=n_estimators, max_features=len(X_train.columns),\n",
    "        max_depth=max_depth)\n",
    "\n",
    "# Fit the RF model with the training set\n",
    "RF.fit(X_train_scaled, y_train)  \n",
    "\n",
    "explainer = shap.TreeExplainer(RF)\n",
    "shap_values = explainer.shap_values(X_train_scaled,approximate=True)\n",
    "        \n",
    "fig = shap.summary_plot(shap_values, X_train_scaled,max_display=10,show=False)\n",
    "\n",
    "# save the image as a file\n",
    "plt.savefig('RF SHAP importances', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an external test set\n",
    "\n",
    "# load the dataset\n",
    "test_set = pd.read_csv(csv_test+'.csv')\n",
    "\n",
    "X_test, y_test = pd.DataFrame(), pd.DataFrame()\n",
    "# only keep the columns used to train the RF model\n",
    "for column in df_model.columns:\n",
    "    if column not in fixed_descriptors+[response_value,'Set',f'Predicted {response_value}']:\n",
    "        X_test[column] = test_set[column]\n",
    "\n",
    "X_test_scaled = (X_test - Xmean) / Xstd\n",
    "\n",
    "y_test = test_set[response_value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model with external test set\n",
    "\n",
    "if prediction_type == 'reg':\n",
    "    # calculate R2, MAE and RMSE for train and test sets\n",
    "    r2_train,mae_train,rmse_train,r2_test,mae_test,rmse_test,y_pred_train,y_pred_test = predictor_workflow(random_init,model_type,parameters_df,X_train_scaled,y_train,X_test_scaled,y_test,prediction_type,train_proportion)\n",
    "    # print stats\n",
    "    print_model_stats(model_type,X_train_scaled,X_test_scaled,r2_train,mae_train,rmse_train,r2_test,mae_test,rmse_test,prediction_type,None,None,'Robert_results_test_set.txt')\n",
    "\n",
    "elif prediction_type == 'clas':\n",
    "    # calculate accuracy, F1 score and MCC for train and test sets\n",
    "    accuracy_train,f1score_train,mcc_train,accuracy_test,f1score_test,mcc_test,y_pred_train,y_pred_test = predictor_workflow(random_init,model_type,parameters_df,X_train_scaled,y_train,X_test_scaled,y_test,prediction_type,train_proportion)\n",
    "    # print stats\n",
    "    print_model_stats(model_type,X_train_scaled,X_test_scaled,accuracy_train,f1score_train,mcc_train,accuracy_test,f1score_test,mcc_test,prediction_type,None,None,'Robert_results_test_set.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training, validation and test sets\n",
    "\n",
    "y_train = training_data[response_value]\n",
    "y_validation = validation_data[response_value]\n",
    "\n",
    "y_pred_train = training_data[f'Predicted {response_value}']\n",
    "y_pred_validation = validation_data[f'Predicted {response_value}']\n",
    "\n",
    "if prediction_type == 'reg':\n",
    "    sb.set(font_scale=1.2, style=\"ticks\") #set styling preferences\n",
    "\n",
    "    Plotdata_train = {'y_train': y_train, 'y_pred_train': y_pred_train} \n",
    "    Plotdata_validation = {'y_validation': y_validation, 'y_pred_validation': y_pred_validation}\n",
    "    Plotdata_test = {'y_test': y_test, 'y_pred_test': y_pred_test}\n",
    "\n",
    "    df_train = pd.DataFrame.from_dict(Plotdata_train)\n",
    "    df_validation = pd.DataFrame.from_dict(Plotdata_validation)\n",
    "    df_test = pd.DataFrame.from_dict(Plotdata_test)\n",
    "\n",
    "    # Build the plot\n",
    "    # Set up some features to plot the dots\n",
    "    color_train = 'b'\n",
    "    color_validation = 'orange'\n",
    "    color_test = 'r'\n",
    "    size = 30\n",
    "    alpha = 1 # from 0 (transparent) to 1 (opaque)\n",
    "\n",
    "    # Create subplot with a certain size and title\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    # Set styling preferences\n",
    "    sb.set(font_scale=1.2, style=\"ticks\")\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    # title of the graph\n",
    "    total_points = len(y_train)+len(y_validation)+len(y_test)\n",
    "    train_proportion = len(y_train)/total_points\n",
    "    validation_proportion = len(y_validation)/total_points\n",
    "    test_proportion = len(y_test)/total_points\n",
    "    ratios =  str(round(train_proportion,2)*100)+':'+str(round(validation_proportion,2)*100)+':'+str(round(test_proportion,2)*100)\n",
    "    title_text = model_type+' model with train:validation ('+ratios+') of '+str(total_points)+' datapoints'\n",
    "    \n",
    "    plt.text(0.5, 1.08, title_text, horizontalalignment='center',\n",
    "         fontsize=14, fontweight='bold', transform = ax.transAxes)\n",
    "\n",
    "    # Plot the data\n",
    "    points_train = ax.scatter(df_train[\"y_train\"], df_train[\"y_pred_train\"],\n",
    "                c = color_train, s = size, edgecolor = 'k', linewidths = 0.8, alpha = alpha, zorder=2)\n",
    "\n",
    "    points_validation = ax.scatter(df_validation[\"y_validation\"], df_validation[\"y_pred_validation\"],\n",
    "                c = color_validation, s = size, edgecolor = 'k', linewidths = 0.8, alpha = alpha, zorder=2)\n",
    "\n",
    "    points_test = ax.scatter(df_test[\"y_test\"], df_test[\"y_pred_test\"],\n",
    "                c = color_test, s = size, edgecolor = 'k', linewidths = 0.8, alpha = alpha, zorder=2)\n",
    "\n",
    "    # Put a legend below current axis\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.17),\n",
    "            fancybox=True, shadow=True, ncol=5, labels=['Training','Validation','Test'])\n",
    "\n",
    "    # Add the regression line with a confidence interval based on the training sets\n",
    "    plot = sb.regplot(\"y_train\", \"y_pred_train\", data=df_train, scatter=False, color=\".1\", \n",
    "                    truncate = True, ax=ax)\n",
    "\n",
    "    # Title of the axis\n",
    "    plot = ax.set(ylabel=f'Predicted {response_value}', xlabel=f'{response_value} from database')\n",
    "    \n",
    "    # Add gridlines\n",
    "    ax.grid(linestyle='--', linewidth=1)\n",
    "\n",
    "    # set limits\n",
    "    size_space = 0.1*abs(min(y_train)-max(y_train))\n",
    "    if min(y_train) < min(y_validation) and min(y_train) < min(y_test):\n",
    "        min_value_graph = min(y_train)-size_space\n",
    "    elif min(y_validation) < min(y_train) and min(y_validation) < min(y_test):\n",
    "        min_value_graph = min(y_validation)-size_space\n",
    "    else:\n",
    "         min_value_graph = min(y_test)-size_space\n",
    "        \n",
    "    if max(y_train) > max(y_validation) and max(y_train) > max(y_test):\n",
    "        max_value_graph = max(y_train)+size_space\n",
    "    elif max(y_validation) > max(y_train) and max(y_validation) > max(y_test):\n",
    "        max_value_graph = max(y_validation)+size_space\n",
    "    else:\n",
    "        max_value_graph = max(y_test)+size_space\n",
    "        \n",
    "    plt.xlim(min_value_graph, max_value_graph)\n",
    "    plt.ylim(min_value_graph, max_value_graph)\n",
    "        \n",
    "    # save the plot a png image, type True\n",
    "    plt.savefig('Predicted vs database values with test.png', dpi=400, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('\\nThe corresponding graph was saved in '+w_dir+'.')\n",
    "\n",
    "elif prediction_type == 'clas':\n",
    "    predictor_model = predictor_model_fun(model_type, parameters_df, random_init, prediction_type)\n",
    "\n",
    "    predictor_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    plot_confusion_matrix(predictor_model, X_test_scaled, y_validation,cmap='Blues') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions in a CSV file\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data[f'Predicted {response_value}'] = y_pred_test\n",
    "test_data['Set'] = 'Test (external)'\n",
    "test_data['Name'] = test_set['Name']\n",
    "test_data[response_value] = y_test\n",
    "\n",
    "combined_data = pd.concat([df_model, test_data])\n",
    "\n",
    "\n",
    "os.remove(f'{csv_training}.csv')\n",
    "export_param_excel = combined_data.to_csv(f'{csv_training}_with_test.csv', index = None, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "cffebf996e571cf1d75cb986e1cc822ae72f8258874d626c15891ac4551cf4a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('DL_CPU': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
